{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c530dcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for 'ueather': ['weather', 'leather', 'feather']\n"
     ]
    }
   ],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "def build_trie(words):\n",
    "    root = TrieNode()\n",
    "    for word in words:\n",
    "        node = root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "    return root\n",
    "\n",
    "def suggest_candidates(trie, misspelled_word):\n",
    "    def dfs(node, path, index, max_changes, current_changes, result):\n",
    "        if node.is_end_of_word and current_changes <= max_changes:\n",
    "            result.append(''.join(path))\n",
    "\n",
    "        if index < len(misspelled_word):\n",
    "            char = misspelled_word[index]\n",
    "            for child_char, child_node in node.children.items():\n",
    "                # Allow character replacement\n",
    "                if char != child_char:\n",
    "                    dfs(child_node, path + [child_char], index + 1, max_changes, current_changes + 1, result)\n",
    "                else:\n",
    "                    dfs(child_node, path + [char], index + 1, max_changes, current_changes, result)\n",
    "\n",
    "    root = trie\n",
    "    result = []\n",
    "    dfs(root, [], 0, 1, 0, result)  # You can adjust the value of max_changes as needed\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "valid_words = [\"summer\", \"weather\", \"leather\", \"feather\", \"black\", \"jacket\", \"nice\"]\n",
    "trie_root = build_trie(valid_words)\n",
    "\n",
    "misspelled_word_1 = \"ueather\"\n",
    "\n",
    "candidates_1 = suggest_candidates(trie_root, misspelled_word_1)\n",
    "\n",
    "\n",
    "print(f\"Candidates for '{misspelled_word_1}': {candidates_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f72309a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in c:\\users\\trishaa\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trishaa\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "     -------------------------------------- 269.5/269.5 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\trishaa\\anaconda3\\envs\\tf\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\trishaa\\anaconda3\\envs\\tf\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cdcbe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidate for 'ueather': weather\n",
      "Best candidate for 'ueather': weather\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def get_ngrams(word, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(word) - n + 1):\n",
    "        ngrams.append(word[i:i + n])\n",
    "    return ngrams\n",
    "\n",
    "def calculate_edit_distance(word1, word2):\n",
    "    return nltk.edit_distance(word1, word2)\n",
    "\n",
    "def suggest_candidates_ngram(valid_words, misspelled_word, n):\n",
    "    misspelled_ngrams = get_ngrams(misspelled_word, n)\n",
    "    min_distance = float('inf')\n",
    "    best_candidate = None\n",
    "\n",
    "    for word in valid_words:\n",
    "        word_ngrams = get_ngrams(word, n)\n",
    "        distance = calculate_edit_distance(misspelled_ngrams, word_ngrams)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_candidate = word\n",
    "\n",
    "    return best_candidate\n",
    "\n",
    "# Example usage\n",
    "valid_words = [\"summer\", \"weather\", \"leather\", \"feather\", \"black\", \"jacket\", \"nice\"]\n",
    "misspelled_word_1 = \"ueather\"\n",
    "misspelled_word_2 = \"ueather\"\n",
    "\n",
    "best_candidate_1 = suggest_candidates_ngram(valid_words, misspelled_word_1, n=2)\n",
    "best_candidate_2 = suggest_candidates_ngram(valid_words, misspelled_word_2, n=2)\n",
    "\n",
    "print(f\"Best candidate for '{misspelled_word_1}': {best_candidate_1}\")\n",
    "print(f\"Best candidate for '{misspelled_word_2}': {best_candidate_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66ba108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentences:\n",
      "During the summer we have the best ueather.\n",
      "I have a black ueather jacket, so nice.\n",
      "\n",
      "Corrected Sentences:\n",
      "during the summer we have the best weather .\n",
      "I have I black weather jacket , so nice .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def get_ngrams(word, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(word) - n + 1):\n",
    "        ngrams.append(word[i:i + n])\n",
    "    return ngrams\n",
    "\n",
    "def calculate_edit_distance(word1, word2):\n",
    "    return nltk.edit_distance(word1, word2)\n",
    "\n",
    "def suggest_candidates_ngram(valid_words, misspelled_word, n):\n",
    "    misspelled_ngrams = get_ngrams(misspelled_word, n)\n",
    "    min_distance = float('inf')\n",
    "    best_candidate = None\n",
    "\n",
    "    for word in valid_words:\n",
    "        word_ngrams = get_ngrams(word, n)\n",
    "        distance = calculate_edit_distance(misspelled_ngrams, word_ngrams)\n",
    "\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_candidate = word\n",
    "\n",
    "    return best_candidate\n",
    "\n",
    "def correct_sentence(sentence, valid_words, n):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.isalpha():  # Ignore punctuation\n",
    "            suggested_word = suggest_candidates_ngram(valid_words, word, n)\n",
    "            corrected_words.append(suggested_word if suggested_word else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "\n",
    "    corrected_sentence = ' '.join(corrected_words)\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage\n",
    "valid_words = [\"black\",\"I\",\"a\",\"weather\", \"leather\", \"jacket\", \"nice\", \"during\", \"the\", \"summer\", \"we\", \"have\", \"best\", \"so\"]\n",
    "input_sentences = [\n",
    "    \"During the summer we have the best ueather.\",\n",
    "    \"I have a black ueather jacket, so nice.\"\n",
    "]\n",
    "\n",
    "n = 2  # You can adjust the value of n as needed\n",
    "\n",
    "output_sentences = [correct_sentence(sentence, valid_words, n) for sentence in input_sentences]\n",
    "\n",
    "print(\"Input Sentences:\")\n",
    "for sentence in input_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nCorrected Sentences:\")\n",
    "for sentence in output_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a9a29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.0-py3-none-any.whl (6.8 MB)\n",
      "     ---------------------------------------- 6.8/6.8 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae6c41cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: The company accepted all the terms.\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "def build_trie(words):\n",
    "    root = TrieNode()\n",
    "    for word in words:\n",
    "        node = root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "    return root\n",
    "\n",
    "def get_candidates_trie(trie, prefix):\n",
    "    node = trie\n",
    "    for char in prefix:\n",
    "        if char in node.children:\n",
    "            node = node.children[char]\n",
    "        else:\n",
    "            # If any character is not found in the Trie, return an empty list\n",
    "            return []\n",
    "\n",
    "    # Traverse the Trie to get all words with the given prefix\n",
    "    candidates = []\n",
    "    get_all_words(node, prefix, candidates)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def get_all_words(node, current_prefix, words):\n",
    "    if node.is_end_of_word:\n",
    "        words.append(current_prefix)\n",
    "\n",
    "    for char, child_node in node.children.items():\n",
    "        get_all_words(child_node, current_prefix + char, words)\n",
    "\n",
    "def choose_correct_word(trie, spell_checker, context, word1, word2):\n",
    "    candidates1 = get_candidates_trie(trie, word1.lower())\n",
    "    candidates2 = get_candidates_trie(trie, word2.lower())\n",
    "\n",
    "    spell_corrected1 = spell_checker.correction(word1)\n",
    "    spell_corrected2 = spell_checker.correction(word2)\n",
    "\n",
    "    all_candidates = candidates1 + candidates2 + [spell_corrected1, spell_corrected2]\n",
    "\n",
    "    if not all_candidates:\n",
    "        return word1  # Default to word1 if neither is found in the dictionary\n",
    "\n",
    "    # Use context keywords for better decision making\n",
    "    context_keywords = {\"excepted\", \"accepted\"}\n",
    "    if any(keyword in context.lower() for keyword in context_keywords):\n",
    "        # If context contains \"accepted,\" prioritize \"accepted\"\n",
    "        return \"accepted\" if \"accepted\" in all_candidates else \"excepted\"\n",
    "    else:\n",
    "        return min(all_candidates, key=lambda x: len(x))\n",
    "\n",
    "# Example\n",
    "dictionary = [\"weather\", \"leather\", \"jacket\", \"nice\", \"during\", \"the\", \"summer\", \"we\", \"have\", \"best\", \"so\", \"principle\", \"principal\", \"excepted\", \"accepted\"]\n",
    "trie = build_trie(dictionary)\n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "spell_checker.word_frequency.load_words(dictionary)\n",
    "\n",
    "sentence = \"The company (excepted/accepted) all the terms.\"\n",
    "words = sentence.split()\n",
    "\n",
    "corrected_sentence = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if \"/\" in word:\n",
    "        # Handle words with multiple choices\n",
    "        choices = word.split(\"/\")\n",
    "        context = \" \".join(words[max(0, i - 2):i + 3])  # Extract context words for decision making\n",
    "        chosen_word = choose_correct_word(trie, spell_checker, context, choices[0], choices[1])\n",
    "        corrected_sentence.append(chosen_word)\n",
    "    else:\n",
    "        corrected_sentence.append(word)\n",
    "\n",
    "result = ' '.join(corrected_sentence)\n",
    "print(\"Corrected Sentence:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9fe865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: Mr Patrick is our new principal\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "def build_trie(words):\n",
    "    root = TrieNode()\n",
    "    for word in words:\n",
    "        node = root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "    return root\n",
    "\n",
    "def get_candidates_trie(trie, prefix):\n",
    "    node = trie\n",
    "    for char in prefix:\n",
    "        if char in node.children:\n",
    "            node = node.children[char]\n",
    "        else:\n",
    "            # If any character is not found in the Trie, return an empty list\n",
    "            return []\n",
    "\n",
    "    # Traverse the Trie to get all words with the given prefix\n",
    "    candidates = []\n",
    "    get_all_words(node, prefix, candidates)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def get_all_words(node, current_prefix, words):\n",
    "    if node.is_end_of_word:\n",
    "        words.append(current_prefix)\n",
    "\n",
    "    for char, child_node in node.children.items():\n",
    "        get_all_words(child_node, current_prefix + char, words)\n",
    "\n",
    "def choose_correct_word(trie, spell_checker, context, word1, word2):\n",
    "    candidates1 = get_candidates_trie(trie, word1.lower())\n",
    "    candidates2 = get_candidates_trie(trie, word2.lower())\n",
    "\n",
    "    spell_corrected1 = spell_checker.correction(word1)\n",
    "    spell_corrected2 = spell_checker.correction(word2)\n",
    "\n",
    "    all_candidates = candidates1 + candidates2 + [spell_corrected1, spell_corrected2]\n",
    "\n",
    "    if not all_candidates:\n",
    "        return word1  # Default to word1 if neither is found in the dictionary\n",
    "\n",
    "    # Use context keywords for better decision making\n",
    "    context_keywords = {\"principle\", \"principal\"}\n",
    "    if any(keyword in context.lower() for keyword in context_keywords):\n",
    "        # If context contains \"accepted,\" prioritize \"accepted\"\n",
    "        return \"principal\" if \"principal\" in all_candidates else \"principle\"\n",
    "    else:\n",
    "        return min(all_candidates, key=lambda x: len(x))\n",
    "\n",
    "# Example\n",
    "dictionary = [\"weather\", \"leather\", \"jacket\", \"nice\", \"during\", \"the\", \"summer\", \"we\", \"have\", \"best\", \"so\", \"principle\", \"principal\"]\n",
    "trie = build_trie(dictionary)\n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "spell_checker.word_frequency.load_words(dictionary)\n",
    "\n",
    "sentence = \"Mr Patrick is our new (principle/principal).\"\n",
    "words = sentence.split()\n",
    "\n",
    "corrected_sentence = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if \"/\" in word:\n",
    "        # Handle words with multiple choices\n",
    "        choices = word.split(\"/\")\n",
    "        context = \" \".join(words[max(0, i - 2):i + 3])  # Extract context words for decision making\n",
    "        chosen_word = choose_correct_word(trie, spell_checker, context, choices[0], choices[1])\n",
    "        corrected_sentence.append(chosen_word)\n",
    "    else:\n",
    "        corrected_sentence.append(word)\n",
    "\n",
    "result = ' '.join(corrected_sentence)\n",
    "print(\"Corrected Sentence:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "013883a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Please don’t keep your dog on the close\n",
      "2. The later is my best friend.\n",
      "3. I need some stationary products for my craftwork.\n",
      "4. The actor excepted the Oscar.\n",
      "5. I will call you later in the evening.\n",
      "6. Covid affects the lungs.\n",
      "7. The council of the ministers were sworn in yesterday.\n",
      "8. Robert too wants to accompany us to the park.\n",
      "9. Mia will council me about choosing fashion as my career.\n",
      "10. The bear at the zoo was very playful.\n",
      "11. The sheep have a lot of fur that keeps them warm.\n",
      "12. The hot spring is at the furthest corner of the street.\n",
      "13. Can you advice me on how to study for exams?\n",
      "14. The team will loose the match if they don’t play well.\n",
      "15. Can you go to the market for me?\n",
      "16. The teachers asked the students to keep quite\n",
      "17. The cheap of garbage should be cleaned immediately.\n",
      "18. This is there house.\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "from nltk.util import ngrams\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "def build_trie(words):\n",
    "    root = TrieNode()\n",
    "    for word in words:\n",
    "        node = root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "    return root\n",
    "\n",
    "def get_candidates_trie(trie, prefix):\n",
    "    node = trie\n",
    "    for char in prefix:\n",
    "        if char in node.children:\n",
    "            node = node.children[char]\n",
    "        else:\n",
    "            # If any character is not found in the Trie, return an empty list\n",
    "            return []\n",
    "\n",
    "    # Traverse the Trie to get all words with the given prefix\n",
    "    candidates = []\n",
    "    get_all_words(node, prefix, candidates)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def get_all_words(node, current_prefix, words):\n",
    "    if node.is_end_of_word:\n",
    "        words.append(current_prefix)\n",
    "\n",
    "    for char, child_node in node.children.items():\n",
    "        get_all_words(child_node, current_prefix + char, words)\n",
    "\n",
    "def choose_correct_word(trie, spell_checker, context, word1, word2):\n",
    "    candidates1 = get_candidates_trie(trie, word1.lower())\n",
    "    candidates2 = get_candidates_trie(trie, word2.lower())\n",
    "\n",
    "    spell_corrected1 = spell_checker.correction(word1)\n",
    "    spell_corrected2 = spell_checker.correction(word2)\n",
    "\n",
    "    all_candidates = candidates1 + candidates2 + [spell_corrected1, spell_corrected2]\n",
    "\n",
    "    if not all_candidates:\n",
    "        return word1  # Default to word1 if neither is found in the dictionary\n",
    "\n",
    "    # Use n-grams to capture context\n",
    "    context_ngrams = set(ngrams(context.lower().split(), 2))\n",
    "    candidate_scores = []\n",
    "\n",
    "    for candidate in all_candidates:\n",
    "        candidate_ngrams = set(ngrams(candidate.lower().split(), 2))\n",
    "        intersection = len(context_ngrams.intersection(candidate_ngrams))\n",
    "        candidate_scores.append((candidate, intersection))\n",
    "\n",
    "    best_candidate = max(candidate_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "    return best_candidate\n",
    "\n",
    "# Example\n",
    "dictionary = [\"lose\", \"loose\", \"later\", \"latter\", \"stationary\", \"stationery\", \"accepted\", \"excepted\", \"council\", \"counsel\",\n",
    "              \"too\", \"to\", \"bear\", \"bare\", \"fur\", \"far\", \"furthest\", \"farthest\", \"advice\", \"advise\", \"loose\", \"lose\",\n",
    "              \"to\", \"too\", \"quiet\", \"quite\", \"heap\", \"hip\", \"there\", \"their\"]\n",
    "\n",
    "trie = build_trie(dictionary)\n",
    "\n",
    "spell_checker = SpellChecker()\n",
    "spell_checker.word_frequency.load_words(dictionary)\n",
    "\n",
    "sentences = [\n",
    "    \"Please don’t keep your dog on the (lose/loose).\",\n",
    "    \"The (later/latter) is my best friend.\",\n",
    "    \"I need some (stationary/stationery) products for my craftwork.\",\n",
    "    \"The actor (excepted/accepted) the Oscar.\",\n",
    "    \"I will call you (later/latter) in the evening.\",\n",
    "    \"Covid (affects/effects) the lungs.\",\n",
    "    \"The (council/counsel) of the ministers were sworn in yesterday.\",\n",
    "    \"Robert (too/to) wants to accompany us to the park.\",\n",
    "    \"Mia will (council/counsel) me about choosing fashion as my career.\",\n",
    "    \"The (bear/bare) at the zoo was very playful.\",\n",
    "    \"The sheep have a lot of (fur/far) that keeps them warm.\",\n",
    "    \"The hot spring is at the (furthest/farthest) corner of the street.\",\n",
    "    \"Can you (advice/advise) me on how to study for exams?\",\n",
    "    \"The team will (loose/lose) the match if they don’t play well.\",\n",
    "    \"Can you go (to/too) the market for me?\",\n",
    "    \"The teachers asked the students to keep (quite/quiet).\",\n",
    "    \"The (heap/hip) of garbage should be cleaned immediately.\",\n",
    "    \"This is (there/their) house.\"\n",
    "]\n",
    "\n",
    "corrected_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    corrected_sentence = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if \"/\" in word:\n",
    "            # Handle words with multiple choices\n",
    "            choices = word.split(\"/\")\n",
    "            context = \" \".join(words[max(0, i - 2):i + 3])  # Extract context words for n-gram comparison\n",
    "            chosen_word = choose_correct_word(trie, spell_checker, context, choices[0], choices[1])\n",
    "            corrected_sentence.append(chosen_word)\n",
    "        else:\n",
    "            corrected_sentence.append(word)\n",
    "\n",
    "    corrected_sentences.append(' '.join(corrected_sentence))\n",
    "\n",
    "for i, corrected_sentence in enumerate(corrected_sentences, start=1):\n",
    "    print(f\"{i}. {corrected_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c181c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
