{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5c2fd8",
   "metadata": {},
   "source": [
    "# Muthu Palaniappan M - 21011101079 NLP LAB EX -2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a6242",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbb17c",
   "metadata": {},
   "source": [
    "1. **Data Gathering**\n",
    "   - Source: Kaggle - https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset\n",
    "   - Labels: Netural, Negative, Positive\n",
    "\n",
    "2. **Data Preprocessing**\n",
    "      - Removing irrelevant characters, symbols, and numbers.\n",
    "      - Tokenization\n",
    "      - Removing stop words\n",
    "      - Lemmatization and stemming to reduce words to their base form. (I did both in this notebook for my Lab ex)\n",
    "\n",
    "3. **Feature Extraction**\n",
    "   - Processed text data into numerical features -> input for machine learning model.\n",
    "   - Bag of Words.\n",
    "   - TF-IDF.\n",
    "   - Skipgram.\n",
    "   - CBOW.\n",
    "\n",
    "4. **Model Choice**\n",
    "      - Naive Bayes classifier.\n",
    "      - Decision Tree with depth 5\n",
    "\n",
    "5. **Model Training**\n",
    "   - Dataset into training and validation sets.\n",
    "   - Train it\n",
    "\n",
    "6. **Model Evaluation**\n",
    "   - Metrics -> accuracy, precision, recall, and F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71810bf1",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "154f2a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "import string as st\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc0a01",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9041012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.csv\",encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0ae373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         selected_text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['selected_text','sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddc323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text sentiment\n",
       "0  I`d have responded, if I were going   neutral\n",
       "1                             Sooo SAD  negative\n",
       "2                          bullying me  negative\n",
       "3                       leave me alone  negative\n",
       "4                        Sons of ****,  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns={\"selected_text\":\"text\"},inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085beab2",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ab0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27480 non-null  object\n",
      " 1   sentiment  27481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 429.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d995a7",
   "metadata": {},
   "source": [
    "#### 1. Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c42f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    removed_text = \"\"\n",
    "    for char in str(text):\n",
    "        if char not in st.punctuation:\n",
    "            removed_text+=char\n",
    "    return removed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d52ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_punc'] = data['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612f6dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I`d have responded, if I were going\n",
      "After: Id have responded if I were going\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['text'][0]}\\nAfter: {data['removed_punc'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe811d",
   "metadata": {},
   "source": [
    "#### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1643c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(text):\n",
    "    text = str(text).lower()\n",
    "    tokens = []\n",
    "    tokens = re.split(\"\\s+\",text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e496505",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tokens'] = data['removed_punc'].apply(convert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da9e2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['id', 'have', 'responded', 'if', 'i', 'were', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens: {data['Tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381aeed",
   "metadata": {},
   "source": [
    "#### 3.Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ba977aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "  return [token for token in tokens if token not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b7f21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['removed_stopwords_tokens'] = data['Tokens'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320c464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'have', 'responded', 'if', 'i', 'were', 'going']\n",
      "After: ['id', 'responded', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['Tokens'][0]}\\nAfter: {data['removed_stopwords_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85150f",
   "metadata": {},
   "source": [
    "#### 4.Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa7bc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb26ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stemming_tokens'] = data['removed_stopwords_tokens'].apply(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3311cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: ['id', 'respond', 'go']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['removed_stopwords_tokens'][0]}\\nAfter: {data['stemming_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f076e",
   "metadata": {},
   "source": [
    "#### 5.Lemma Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3925125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lema_tokens(tokens):\n",
    "    word_net = WordNetLemmatizer()\n",
    "    tokens = [word_net.lemmatize(tok) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6acbed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_tokens'] = data['removed_stopwords_tokens'].apply(lema_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f203e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: ['id', 'responded', 'going']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['removed_stopwords_tokens'][0]}\\nAfter: {data['lemma_tokens'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f259c",
   "metadata": {},
   "source": [
    "#### 6. Return Pre-Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d92e256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequence(tokens):\n",
    "  return \" \".join([token for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196a43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pre_processed_text'] = data['lemma_tokens'].apply(return_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91793e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['id', 'responded', 'going']\n",
      "After: id responded going\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {data['lemma_tokens'][0]}\\nAfter: {data['pre_processed_text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09e052cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ea4aa",
   "metadata": {},
   "source": [
    "### Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004d200",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b96855",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(data['pre_processed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5592545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 17424)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616d4e10",
   "metadata": {},
   "source": [
    "##### Impact on BoW\n",
    "- BoW may struggle with capturing semantic meaning and context, leading to misclassification. \n",
    "- It treats each word independently, ignoring word relationships.\n",
    "- OOV Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05e1fb",
   "metadata": {},
   "source": [
    "#### Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "70aeef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['pre_processed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bbe8adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2a85b",
   "metadata": {},
   "source": [
    "###### Impact on TF-IDF\n",
    "- While TF-IDF addresses some BoW limitations by giving more weight to important words, it still doesn't capture word relationships and semantics well.\n",
    "- OOV Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b170356",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17e7e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(data['pre_processed_text'].values.tolist(), vector_size=100, window=5, min_count=2, sg=0)\n",
    "vocab = cbow.wv.index_to_key\n",
    "\n",
    "def get_mean_vector(model, sentence):\n",
    "    words = [word for word in sentence if word in vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    return np.zeros((100,))\n",
    "\n",
    "cbow_array = []\n",
    "for sentence in data['pre_processed_text'].values.tolist():\n",
    "    cbow_array.append(get_mean_vector(cbow, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1abd9214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_array = np.array(cbow_array)\n",
    "cbow_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44099ee",
   "metadata": {},
   "source": [
    "#### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d77621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = Word2Vec(data['pre_processed_text'].values.tolist(), vector_size=100, window=5, min_count=2, sg=1)\n",
    "vocab = sg.wv.index_to_key\n",
    "\n",
    "def get_mean_vector(model, sentence):\n",
    "    words = [word for word in sentence if word in vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    return np.zeros((100,))\n",
    "\n",
    "sg_array = []\n",
    "for sentence in data['pre_processed_text'].values.tolist():\n",
    "    sg_array.append(get_mean_vector(sg, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b561c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_array = np.array(sg_array)\n",
    "sg_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b10d8",
   "metadata": {},
   "source": [
    "###### Impact on Word2Vec\n",
    "- These models are better at capturing semantic relationships and context, reducing misclassification related to word semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf470f6",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d21c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "data['sentiment'] = lb.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "254d6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ddbcc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_test_bow, y_train_bow, y_test_bow = train_test_split(count_matrix, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c7ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c3fd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cbow, x_test_cbow, y_train_cbow, y_test_cbow = train_test_split(cbow_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aefebbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_skg, x_test_skg, y_train_skg, y_test_skg = train_test_split(sg_array, y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18aa77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Shapes:\n",
      "x_train_bow shape: (21984, 17424)\n",
      "x_test_bow shape: (5496, 17424)\n",
      "y_train_bow shape: (21984,)\n",
      "y_test_bow shape: (5496,)\n",
      "=======================\n",
      "\n",
      "TF-IDF Shapes:\n",
      "x_train_tfidf shape: (21984, 17424)\n",
      "x_test_tfidf shape: (5496, 17424)\n",
      "y_train_tfidf shape: (21984,)\n",
      "y_test_tfidf shape: (5496,)\n",
      "=========================\n",
      "\n",
      "Continuous Bag of Words (CBOW) Shapes:\n",
      "x_train_cbow shape: (21984, 100)\n",
      "x_test_cbow shape: (5496, 100)\n",
      "y_train_cbow shape: (21984,)\n",
      "y_test_cbow shape: (5496,)\n",
      "========================\n",
      "\n",
      "Skip-Gram Shapes:\n",
      "x_train_skg shape: (21984, 100)\n",
      "x_test_skg shape: (5496, 100)\n",
      "y_train_skg shape: (21984,)\n",
      "y_test_skg shape: (5496,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words (BoW) Shapes:\")\n",
    "print(\"x_train_bow shape:\", x_train_bow.shape)\n",
    "print(\"x_test_bow shape:\", x_test_bow.shape)\n",
    "print(\"y_train_bow shape:\", y_train_bow.shape)\n",
    "print(\"y_test_bow shape:\", y_test_bow.shape)\n",
    "print(\"=======================\")\n",
    "print(\"\\nTF-IDF Shapes:\")\n",
    "print(\"x_train_tfidf shape:\", x_train_tfidf.shape)\n",
    "print(\"x_test_tfidf shape:\", x_test_tfidf.shape)\n",
    "print(\"y_train_tfidf shape:\", y_train_tfidf.shape)\n",
    "print(\"y_test_tfidf shape:\", y_test_tfidf.shape)\n",
    "print(\"=========================\")\n",
    "print(\"\\nContinuous Bag of Words (CBOW) Shapes:\")\n",
    "print(\"x_train_cbow shape:\", x_train_cbow.shape)\n",
    "print(\"x_test_cbow shape:\", x_test_cbow.shape)\n",
    "print(\"y_train_cbow shape:\", y_train_cbow.shape)\n",
    "print(\"y_test_cbow shape:\", y_test_cbow.shape)\n",
    "print(\"========================\")\n",
    "print(\"\\nSkip-Gram Shapes:\")\n",
    "print(\"x_train_skg shape:\", x_train_skg.shape)\n",
    "print(\"x_test_skg shape:\", x_test_skg.shape)\n",
    "print(\"y_train_skg shape:\", y_train_skg.shape)\n",
    "print(\"y_test_skg shape:\", y_test_skg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294bd3f",
   "metadata": {},
   "source": [
    "### Model Builing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "066ad619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_decision_tree(x_train, x_test, y_train, y_test, representation):\n",
    "    \n",
    "    dtclassifier = DecisionTreeClassifier(random_state=9,max_depth=5)\n",
    "    dtclassifier.fit(x_train, y_train)\n",
    "    y_pred = dtclassifier.predict(x_test)\n",
    "\n",
    "    print(f\"\\nMetrics for {representation}:\")\n",
    "    print(f\"Model Score: {dtclassifier.score(x_train,y_train)}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b19be50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_navie_bayes(x_train, x_test, y_train, y_test, representation):\n",
    "    \n",
    "    nbclassifier = MultinomialNB()\n",
    "    nbclassifier.fit(x_train, y_train)\n",
    "    y_pred = nbclassifier.predict(x_test)\n",
    "\n",
    "    print(f\"\\nMetrics for {representation}:\")\n",
    "    print(f\"Model Score: {nbclassifier.score(x_train,y_train)}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    return nbclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8637bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for BoW:\n",
      "Model Score: 0.4932678311499272\n",
      "Accuracy: 0.4798034934497817\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.00      0.00      1548\n",
      "           1       0.43      0.94      0.59      2182\n",
      "           2       0.79      0.33      0.46      1766\n",
      "\n",
      "    accuracy                           0.48      5496\n",
      "   macro avg       0.61      0.42      0.35      5496\n",
      "weighted avg       0.59      0.48      0.39      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_bow, x_test_bow, y_train_bow, y_test_bow, \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5be64ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for TF-IDF:\n",
      "Model Score: 0.5020469432314411\n",
      "Accuracy: 0.4885371179039301\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1548\n",
      "           1       0.44      0.99      0.61      2182\n",
      "           2       0.91      0.30      0.45      1766\n",
      "\n",
      "    accuracy                           0.49      5496\n",
      "   macro avg       0.78      0.43      0.35      5496\n",
      "weighted avg       0.75      0.49      0.39      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35045729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for CBOW:\n",
      "Model Score: 0.6676219068413392\n",
      "Accuracy: 0.6519286754002911\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.52      0.56      1548\n",
      "           1       0.67      0.80      0.73      2182\n",
      "           2       0.66      0.58      0.62      1766\n",
      "\n",
      "    accuracy                           0.65      5496\n",
      "   macro avg       0.65      0.63      0.64      5496\n",
      "weighted avg       0.65      0.65      0.65      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_cbow, x_test_cbow, y_train_cbow, y_test_cbow, \"CBOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77a8027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Skip-Gram:\n",
      "Model Score: 0.6605258369723436\n",
      "Accuracy: 0.6486535662299855\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.55      0.58      1548\n",
      "           1       0.62      0.83      0.71      2182\n",
      "           2       0.78      0.51      0.62      1766\n",
      "\n",
      "    accuracy                           0.65      5496\n",
      "   macro avg       0.67      0.63      0.63      5496\n",
      "weighted avg       0.67      0.65      0.64      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_decision_tree(x_train_skg, x_test_skg, y_train_skg, y_test_skg, \"Skip-Gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c8bf9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for BoW:\n",
      "Model Score: 0.8563045851528385\n",
      "Accuracy: 0.7530931586608443\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.69      1548\n",
      "           1       0.71      0.81      0.76      2182\n",
      "           2       0.78      0.80      0.79      1766\n",
      "\n",
      "    accuracy                           0.75      5496\n",
      "   macro avg       0.76      0.74      0.75      5496\n",
      "weighted avg       0.76      0.75      0.75      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc_1 = train_and_evaluate_navie_bayes(x_train_bow, x_test_bow, y_train_bow, y_test_bow, \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dee8364d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for Tf-IDF:\n",
      "Model Score: 0.8605349344978166\n",
      "Accuracy: 0.774745269286754\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.59      0.70      1548\n",
      "           1       0.71      0.89      0.79      2182\n",
      "           2       0.82      0.79      0.81      1766\n",
      "\n",
      "    accuracy                           0.77      5496\n",
      "   macro avg       0.80      0.76      0.77      5496\n",
      "weighted avg       0.79      0.77      0.77      5496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc_2 = train_and_evaluate_navie_bayes(x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf, \"Tf-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970fc06",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6548aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"What is not to like about this product.\",\n",
    "    \"Not bad.\",\n",
    "    \"Not an issue.\",\n",
    "    \"Not buggy.\",\n",
    "    \"Not happy.\",\n",
    "    \"Not user-friendly.\",\n",
    "    \"Not good.\",\n",
    "    \"Is it any good?\",\n",
    "    \"I do not dislike horror movies.\",\n",
    "    \"Disliking horror movies is not uncommon.\",\n",
    "    \"Sometimes I really hate the show.\",\n",
    "    \"I love having to wait two months for the next series to come out!\",\n",
    "    \"The final episode was surprising with a terrible twist at the end.\",\n",
    "    \"The film was easy to watch but I would not recommend it to my friends.\",\n",
    "    \"I LOL’d at the end of the cake scene\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94e9d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is not to like about this product.: Positive\n",
      "Not bad.: Negative\n",
      "Not an issue.: Negative\n",
      "Not buggy.: Neutral\n",
      "Not happy.: Positive\n",
      "Not user-friendly.: Positive\n",
      "Not good.: Positive\n",
      "Is it any good?: Positive\n",
      "I do not dislike horror movies.: Negative\n",
      "Disliking horror movies is not uncommon.: Negative\n",
      "Sometimes I really hate the show.: Neutral\n",
      "I love having to wait two months for the next series to come out!: Neutral\n",
      "The final episode was surprising with a terrible twist at the end.: Neutral\n",
      "The film was easy to watch but I would not recommend it to my friends.: Neutral\n",
      "I LOL’d at the end of the cake scene: Neutral\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    preprocessed_text = \" \".join(simple_preprocess(text))\n",
    "    transformed_text = tfidf.transform([preprocessed_text]).toarray()\n",
    "    prediction = nbc_1.predict(transformed_text)[0]\n",
    "    \n",
    "    if prediction == 0:\n",
    "        print(f\"{text}: Negative\")\n",
    "    elif prediction == 1:\n",
    "        print(f\"{text}: Neutral\")\n",
    "    elif prediction == 2:\n",
    "        print(f\"{text}: Positive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
